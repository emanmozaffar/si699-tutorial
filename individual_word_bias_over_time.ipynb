{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "datafile = \"moviedata.csv\"\n",
    "movies = pd.read_csv(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "movies['text'] = movies['text'].apply(gensim.utils.simple_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define word lists\n",
    "competence_words = ['precocious', 'resourceful', 'inquisitive', 'sagacious', 'inventive', 'astute', 'adaptable', 'reflective', 'discerning', 'intuitive', 'inquiring', 'judicious', 'analytical', 'luminous', 'venerable', 'imaginative', 'shrewd', 'thoughtful', 'sage',\n",
    "'smart', 'ingenious', 'clever', 'brilliant', 'logical', 'intelligent', 'apt', 'genius', 'wise']  \n",
    "physical_appearance_words = ['alluring', 'voluptuous', 'blushing', 'homely', 'plump', 'sensual', 'gorgeous', 'slim', 'bald', 'athletic', 'fashionable', 'stout', 'ugly', 'muscular', 'slender', 'feeble', 'handsome', 'healthy', 'attractive', 'fat', 'weak', 'thin', 'pretty',\n",
    "'beautiful', 'strong']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = [\n",
    "    \"word2vec-movies_1921_1930-text.model\",\n",
    "    \"word2vec-movies_1931_1940-text.model\",\n",
    "    \"word2vec-movies_1941_1950-text.model\",\n",
    "    \"word2vec-movies_1951_1960-text.model\",\n",
    "    \"word2vec-movies_1961_1970-text.model\",\n",
    "    \"word2vec-movies_1971_1980-text.model\",\n",
    "    \"word2vec-movies_1981_1990-text.model\",\n",
    "    \"word2vec-movies_1991_2000-text.model\",\n",
    "    \"word2vec-movies_2001_2010-text.model\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [gensim.models.Word2Vec.load(model_file) for model_file in model_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute embedding bias for a word in a specific model\n",
    "def compute_embedding_bias(model, word):\n",
    "    # Check if the word exists in the vocabulary of the model\n",
    "    if word not in model.wv:\n",
    "        print(f\"Word '{word}' not found in the vocabulary.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate the average vector for the word\n",
    "    word_vector = model.wv[word]\n",
    "\n",
    "    # Initialize variables to store distances from the word vector to gender-specific words\n",
    "    distance_to_men = []\n",
    "    distance_to_women = []\n",
    "\n",
    "    # Define gender-specific words\n",
    "    gender_words = {'man', 'men', 'woman', 'women', 'male', 'female'}  # You can extend this list\n",
    "\n",
    "    # Calculate distances from the word vector to gender-specific words\n",
    "    for gender_word in gender_words:\n",
    "        if gender_word in model.wv:\n",
    "            gender_word_vector = model.wv[gender_word]\n",
    "            # Calculate cosine similarity between the word vector and gender-specific word vectors\n",
    "            similarity = np.dot(word_vector, gender_word_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(gender_word_vector))\n",
    "            if 'man' in gender_word or 'male' in gender_word:\n",
    "                distance_to_men.append(1 - similarity)  # 1 minus similarity gives distance\n",
    "            else:\n",
    "                distance_to_women.append(1 - similarity)\n",
    "\n",
    "    # Calculate average distances to men and women\n",
    "    avg_distance_to_men = np.mean(distance_to_men)\n",
    "    avg_distance_to_women = np.mean(distance_to_women)\n",
    "\n",
    "    # Calculate embedding bias as the difference between distances to men and women\n",
    "    embedding_bias = avg_distance_to_women - avg_distance_to_men\n",
    "\n",
    "    return embedding_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute embedding bias for a word over time\n",
    "def compute_bias_over_time(word, models):\n",
    "    biases = []\n",
    "    for model in models:\n",
    "        # Compute embedding bias for the word in each model\n",
    "        # You need to implement how to compute the bias based on your logic\n",
    "        bias = compute_embedding_bias(model, word)\n",
    "        biases.append(bias)\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'precocious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'sagacious' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'inventive' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'adaptable' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'discerning' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'judicious' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n",
      "Word 'venerable' not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "# Compute biases for competence words over time\n",
    "competence_biases_over_time = {}\n",
    "for word in competence_words:\n",
    "    biases = compute_bias_over_time(word, models)\n",
    "    competence_biases_over_time[word] = biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Most of the competence words are not present in the corpus (movie lines) used to train the Word2Vec models.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute biases for physical appearance words over time\n",
    "physical_appearance_biases_over_time = {}\n",
    "for word in physical_appearance_words:\n",
    "    biases = compute_bias_over_time(word, models)\n",
    "    physical_appearance_biases_over_time[word] = biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already computed the biases over time for the competence and physical appearance words\n",
    "# competence_biases_over_time and physical_appearance_biases_over_time are dictionaries containing biases for each word over time\n",
    "\n",
    "# Extract the years from 1960 to 1990\n",
    "years = range(1960, 1991)\n",
    "\n",
    "# Prepare the data for competence words regression\n",
    "competence_regression_data = {'Year': [], 'Word': [], 'Bias': []}\n",
    "\n",
    "for word, biases in competence_biases_over_time.items():\n",
    "    for year, bias in zip(years, biases):\n",
    "        competence_regression_data['Year'].append(year)\n",
    "        competence_regression_data['Word'].append(word)\n",
    "        competence_regression_data['Bias'].append(bias)\n",
    "\n",
    "competence_df = pd.DataFrame(competence_regression_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Word</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960</td>\n",
       "      <td>precocious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1961</td>\n",
       "      <td>precocious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962</td>\n",
       "      <td>precocious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1963</td>\n",
       "      <td>precocious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1964</td>\n",
       "      <td>precocious</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1964</td>\n",
       "      <td>wise</td>\n",
       "      <td>0.230830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1965</td>\n",
       "      <td>wise</td>\n",
       "      <td>0.212473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1966</td>\n",
       "      <td>wise</td>\n",
       "      <td>0.241944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1967</td>\n",
       "      <td>wise</td>\n",
       "      <td>0.186649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1968</td>\n",
       "      <td>wise</td>\n",
       "      <td>0.176048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year        Word      Bias  Constant\n",
       "0    1960  precocious       NaN         1\n",
       "1    1961  precocious       NaN         1\n",
       "2    1962  precocious       NaN         1\n",
       "3    1963  precocious       NaN         1\n",
       "4    1964  precocious       NaN         1\n",
       "..    ...         ...       ...       ...\n",
       "247  1964        wise  0.230830         1\n",
       "248  1965        wise  0.212473         1\n",
       "249  1966        wise  0.241944         1\n",
       "250  1967        wise  0.186649         1\n",
       "251  1968        wise  0.176048         1\n",
       "\n",
       "[252 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression results for competence words:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   Bias   R-squared:                         nan\n",
      "Model:                            OLS   Adj. R-squared:                    nan\n",
      "Method:                 Least Squares   F-statistic:                       nan\n",
      "Date:                Sun, 11 Feb 2024   Prob (F-statistic):                nan\n",
      "Time:                        22:33:22   Log-Likelihood:                    nan\n",
      "No. Observations:                 252   AIC:                               nan\n",
      "Df Residuals:                     250   BIC:                               nan\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Constant          nan        nan        nan        nan         nan         nan\n",
      "Year              nan        nan        nan        nan         nan         nan\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                     nan\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                  nan\n",
      "Skew:                             nan   Prob(JB):                          nan\n",
      "Kurtosis:                         nan   Cond. No.                     1.49e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.49e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Add a constant term for the regression\n",
    "competence_df['Constant'] = 1\n",
    "\n",
    "# Perform regression for competence words\n",
    "competence_model = sm.OLS(competence_df['Bias'], competence_df[['Constant', 'Year']])\n",
    "competence_results = competence_model.fit()\n",
    "print(\"Regression results for competence words:\")\n",
    "print(competence_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Prepare the data for physical appearance words regression\n",
    "physical_appearance_regression_data = {'Year': [], 'Word': [], 'Bias': []}\n",
    "\n",
    "for word, biases in physical_appearance_biases_over_time.items():\n",
    "    for year, bias in zip(years, biases):\n",
    "        physical_appearance_regression_data['Year'].append(year)\n",
    "        physical_appearance_regression_data['Word'].append(word)\n",
    "        physical_appearance_regression_data['Bias'].append(bias)\n",
    "\n",
    "physical_appearance_df = pd.DataFrame(physical_appearance_regression_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Word</th>\n",
       "      <th>Bias</th>\n",
       "      <th>Constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960</td>\n",
       "      <td>alluring</td>\n",
       "      <td>0.094286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1961</td>\n",
       "      <td>alluring</td>\n",
       "      <td>0.228762</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962</td>\n",
       "      <td>alluring</td>\n",
       "      <td>0.154791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1963</td>\n",
       "      <td>alluring</td>\n",
       "      <td>0.216287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1964</td>\n",
       "      <td>alluring</td>\n",
       "      <td>0.074552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1964</td>\n",
       "      <td>strong</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1965</td>\n",
       "      <td>strong</td>\n",
       "      <td>0.025215</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1966</td>\n",
       "      <td>strong</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1967</td>\n",
       "      <td>strong</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1968</td>\n",
       "      <td>strong</td>\n",
       "      <td>0.068792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year      Word      Bias  Constant\n",
       "0    1960  alluring  0.094286         1\n",
       "1    1961  alluring  0.228762         1\n",
       "2    1962  alluring  0.154791         1\n",
       "3    1963  alluring  0.216287         1\n",
       "4    1964  alluring  0.074552         1\n",
       "..    ...       ...       ...       ...\n",
       "220  1964    strong  0.014492         1\n",
       "221  1965    strong  0.025215         1\n",
       "222  1966    strong  0.002842         1\n",
       "223  1967    strong  0.005537         1\n",
       "224  1968    strong  0.068792         1\n",
       "\n",
       "[225 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_appearance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression results for physical appearance words:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   Bias   R-squared:                       0.006\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     1.252\n",
      "Date:                Sun, 11 Feb 2024   Prob (F-statistic):              0.264\n",
      "Time:                        22:33:22   Log-Likelihood:                 258.13\n",
      "No. Observations:                 225   AIC:                            -512.3\n",
      "Df Residuals:                     223   BIC:                            -505.4\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Constant       4.5404      3.913      1.160      0.247      -3.172      12.252\n",
      "Year          -0.0022      0.002     -1.119      0.264      -0.006       0.002\n",
      "==============================================================================\n",
      "Omnibus:                        8.370   Durbin-Watson:                   0.640\n",
      "Prob(Omnibus):                  0.015   Jarque-Bera (JB):                6.657\n",
      "Skew:                          -0.323   Prob(JB):                       0.0358\n",
      "Kurtosis:                       2.458   Cond. No.                     1.49e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.49e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Add a constant term for the regression\n",
    "physical_appearance_df['Constant'] = 1\n",
    "\n",
    "# Perform regression for physical appearance words\n",
    "physical_appearance_model = sm.OLS(physical_appearance_df['Bias'], physical_appearance_df[['Constant', 'Year']])\n",
    "physical_appearance_results = physical_appearance_model.fit()\n",
    "print(\"\\nRegression results for physical appearance words:\")\n",
    "print(physical_appearance_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- R-squared: The R-squared value indicates the proportion of variance in the dependent variable (embedding bias) explained by the independent variable (years). In this case, the R-squared value is 0.006, indicating that ```only a small proportion of the variance in embedding bias is explained by the years.```\n",
    "\n",
    "- Adjusted R-squared: The adjusted R-squared value adjusts the R-squared value for the number of predictors in the model. It is similar to R-squared but penalizes the addition of unnecessary predictors. In this case, the adjusted R-squared value is 0.001, which is very close to zero, suggesting that the independent variable (years) may not be a good predictor of embedding bias for physical appearance words.\n",
    "\n",
    "- F-statistic and Prob (F-statistic): The F-statistic tests the overall significance of the regression model. The associated p-value (Prob (F-statistic)) indicates the probability of obtaining an F-statistic as extreme as the one observed if the null hypothesis (that all regression coefficients are zero) is true. In this case, ```the p-value is 0.264, which is greater than the typical significance level of 0.05. Therefore, we fail to reject the null hypothesis, suggesting that the regression model may not be statistically significant.```\n",
    "\n",
    "- Coefficients: The coefficients represent the estimated effects of the independent variable (years) on the dependent variable (embedding bias). In this case, the coefficient for the year variable is -0.0022, indicating a slight negative trend over time. However, the coefficient is not statistically significant at the 0.05 level, as indicated by the p-value (P>|t|) of 0.264.\n",
    "\n",
    "Overall, based on these regression results, it appears that ```there is little to no significant relationship between the years and the embedding bias for physical appearance words.``` The model does not provide strong evidence to suggest that the embedding bias for physical appearance words has changed significantly over time from the 1960s to 1990s.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
